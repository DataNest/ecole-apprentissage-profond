{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Theano\n",
    "\n",
    "This notebook contains the code snippets from the slides, so you can execute them and tinker with those examples.\n",
    "\n",
    "To execute a cell: Ctrl-Enter.\n",
    "\n",
    "The code was executed with the default configuration of Theano: `floatX=float64`, `device=cpu` and the configuration for GPU `floatX=float32,device=cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS'] = 'floatX=float64,device=cpu,mode=FAST_RUN'\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cuda,mode=FAST_RUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX TITAN Black (0000:02:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano concepts\n",
    "\n",
    "## Symbolic input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbolic inputs that you operate on are **Variables** and what you get from applying various **Ops** to these inputs are also Variables. A Variable is the main data structure you work with. A **Type** in Theano represents a set of constraints on potential data objects. These constraints allow Theano to tailor C code to handle them and to statically optimize the computation graph. The Type of both `x` and `y` is `dmatrix`. Here is [the complete list of types](http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **Op** defines a certain computation on some types of inputs, producing some types of outputs. From a list of input Variables and an Op, you can build an **Apply** node representing the application of the Op to the inputs.\n",
    "\n",
    "An Apply node is a type of internal node used to represent a computation graph.\n",
    "It represents the application of an Op on one or more inputs, where each input is a Variable. By convention, each Op is responsible for knowing how to build an Apply node from a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![apply.png](http://deeplearning.net/software/theano/_images/apply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## theano.function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`theano.function` is the interface for compiling graphs into callable objects. When `theano.function` is executed, the computation graph is optimized and theano generates an efficient code in C (with calls to CUDA if the gpu flag is set). This is totally transparent to the user, except for the different compilation modes.\n",
    "The mode argument controls the sort of optimizations that will be applied to the graph, and the way the optimized graph will be evaluated. These modes are:\n",
    "- `FAST_COMPILE`: Apply just a few graph optimizations and only use Python implementations. So GPU is disabled.\n",
    "\n",
    "- `FAST_RUN`: Apply all optimizations and use C implementations where possible. (DEFAULT)\n",
    "\n",
    "- `DebugMode`: Verify the correctness of all optimizations, and compare C and Python implementations. This mode can take much longer than the other modes, but can identify several kinds of problems.\n",
    "\n",
    "The default is typically `FAST_RUN` but this can be changed in `theano.config.mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26539643,  1.27784336, -0.44830998]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(1, 3)\n",
    "b = np.random.randn(1, 3)\n",
    "\n",
    "# theano.function([inputs], [outputs])\n",
    "f = theano.function([x, y], z, mode='FAST_RUN')\n",
    "f(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Shared Variable** is a hybrid symbolic and non-symbolic variable whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. The value can be accessed and modified by the .get_value() and .set_value() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_val = np.random.randn(4, 3)\n",
    "b_val = np.ones(3)\n",
    "\n",
    "W = theano.shared(W_val)\n",
    "b = theano.shared(b_val)\n",
    "\n",
    "W.name = 'W'\n",
    "b.name = 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6260868   0.99128932  0.60516261]\n",
      " [-0.3167549  -0.86360091  0.27229104]\n",
      " [-0.77053418 -0.29000988  1.55417391]\n",
      " [-1.6184249   0.36977367  1.69348389]]\n",
      "Before  [ 1.  1.  1.]\n",
      "After  [ 1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(W.get_value())\n",
    "print('Before ', b.get_value())\n",
    "# b.set_value(1) --  Type error, must be a numpy array of shape (3,)\n",
    "# b.set_value(np.array([[1,2],[3,4]])) # Type error, must be a numpy array of shape (3,)\n",
    "b.set_value(np.array([1,2,3]))\n",
    "print('After ', b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared variables can be used to represent an internal state of a function. In order to modify this internal state, the function has an argument called `updates`, which takes an iterable over pairs (shared_variable, new_expression) List, tuple or dict.\n",
    "\n",
    "Note in the following that `state` is an implicit input of the function `accumulator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = theano.shared(0)\n",
    "inc = T.iscalar('inc')\n",
    "accumulator = theano.function([inc], state, updates=[(state, state+inc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is evaluated and then, the update mechanism is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call to accumulator 0:\n",
      "Second call to accumulator 1:\n",
      "Third call to accumulator 11:\n",
      "Fourth call to accumulator 111:\n"
     ]
    }
   ],
   "source": [
    "print('First call to accumulator {}:'.format(accumulator(1)))\n",
    "print('Second call to accumulator {}:'.format(accumulator(10)))\n",
    "print('Third call to accumulator {}:'.format(accumulator(100)))\n",
    "print('Fourth call to accumulator {}:'.format(accumulator(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may happen that you expressed some formula using a shared variable, but you do not want to use its value. In this case, you can use the givens parameter of function which replaces a particular node in a graph for the purpose of one particular function. The givens parameter can be used to replace any symbolic variable, not just a shared variable. You can replace constants, and expressions, in general. Be careful though, not to allow the expressions introduced by a givens substitution to be co-dependent, the order of substitution is not defined, so the substitutions have to work in any order.\n",
    "\n",
    "In practice, a good way of thinking about the givens is as a mechanism that allows you to replace any part of your formula with a different expression that evaluates to a tensor of same shape and dtype. [[reference]](http://deeplearning.net/software/theano/tutorial/examples.html#basictutexamples)\n",
    "\n",
    "In the following, we create a function that takes a scalar `foo`, replace temporarily the variable `state` and return its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "foo = T.scalar(dtype=state.dtype)\n",
    "accumulator2 = theano.function([foo], state, givens=[(state, foo)])\n",
    "print(accumulator2(1))\n",
    "print(state.get_value())  # old state still there, but we didn't use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A regression toy example\n",
    "## Build a simple model\n",
    "The following is a simple linear transformation (out = Wx +b) followed by a nonlinearity (theano.sigmoid). Note that in this example, we are using `allow_input_downcast=True` in order to avoid an error associated to downcasting x_val from a `float64` to a `float32`. Without this parameter, `x_val` must be casted explicitly: `x_val = np.random.rand(4).astype(np.float32)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72872235  0.66253532  0.78568   ]\n"
     ]
    }
   ],
   "source": [
    "x = T.fvector('x')\n",
    "\n",
    "W_val = np.random.randn(4, 3)\n",
    "b_val = np.ones(3)\n",
    "W = theano.shared(W_val, name = 'W')\n",
    "b = theano.shared(b_val, name = 'b')\n",
    "\n",
    "#dot = T.dot(W, x)\n",
    "dot = T.dot(x, W)\n",
    "out = T.nnet.sigmoid(dot + b)\n",
    "\n",
    "predict = theano.function([x], out, allow_input_downcast=True)\n",
    "x_val = np.random.rand(4)\n",
    "print(predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, we define a cost function that will evaluate how far the model is from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7907433736413003\n"
     ]
    }
   ],
   "source": [
    "y = T.vector('y')\n",
    "C = ((out - y) ** 2).sum()\n",
    "C.name = 'C'\n",
    "error = theano.function([out, y], C, allow_input_downcast=True)\n",
    "\n",
    "y_val = np.random.uniform(size=3)\n",
    "print(error([0.942, 0.737, 0.676], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is defined, we can compute the gradient of the cost C w.r.t some parameters (W,b). The gradient must be applied to a scalar expression, e.g., the cost C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theano.grad(exp, [Variable])\n",
    "dC_dW, dC_db = theano.grad(C, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the gradients, we define the gradient descent update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = theano.shared(0.1, name='eta')\n",
    "upd_W = W - eta * dC_dW\n",
    "upd_b = b - eta * dC_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compile the expressions and the update rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.]\n",
      "[[-1.67929044 -0.92487517  0.20092022]\n",
      " [ 0.1836611  -0.78844311  1.55006705]\n",
      " [-0.2869607   0.69831984  0.89062235]\n",
      " [ 0.33461893  0.53819034 -1.1150567 ]]\n"
     ]
    }
   ],
   "source": [
    "train = theano.function([x, y], C, updates=[(W, upd_W), (b, upd_b)], givens=[(eta,0.1)], allow_input_downcast=True)\n",
    "#train = theano.function([x, y], C, updates=[(W, upd_W), (b, upd_b)], allow_input_downcast=True)\n",
    "print(b.get_value())\n",
    "print(W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate the gradient descent update rule in order to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 0.42685626131334453 at iteration 0\n",
      "Cost 0.4119742716127751 at iteration 1\n",
      "Cost 0.3972164436032738 at iteration 2\n",
      "Cost 0.3826137152091903 at iteration 3\n",
      "Cost 0.368196317732718 at iteration 4\n",
      "Cost 0.35399343874502437 at iteration 5\n",
      "Cost 0.3400329018166908 at iteration 6\n",
      "Cost 0.326340870465682 at iteration 7\n",
      "Cost 0.3129415830299921 at iteration 8\n",
      "Cost 0.29985712424798006 at iteration 9\n",
      "Cost 0.2871072381847368 at iteration 10\n",
      "Cost 0.2747091858230399 at iteration 11\n",
      "Cost 0.2626776491998973 at iteration 12\n",
      "Cost 0.25102468248122345 at iteration 13\n",
      "Cost 0.23975970890022497 at iteration 14\n",
      "Cost 0.2288895611122897 at iteration 15\n",
      "Cost 0.21841856130757256 at iteration 16\n",
      "Cost 0.20834863642770987 at iteration 17\n",
      "Cost 0.19867946309477827 at iteration 18\n",
      "Cost 0.1894086363994644 at iteration 19\n",
      "Cost 0.18053185651240647 at iteration 20\n",
      "Cost 0.17204312716051062 at iteration 21\n",
      "Cost 0.16393496031620433 at iteration 22\n",
      "Cost 0.156198581938722 at iteration 23\n",
      "Cost 0.14882413423330323 at iteration 24\n",
      "[ 0.5608996   0.62022332  1.10268161]\n",
      "[[-1.77668039 -1.00910745  0.22369441]\n",
      " [-0.1124265  -1.04452843  1.61930577]\n",
      " [-0.29854286  0.68830246  0.89333079]\n",
      " [ 0.01399602  0.2608845  -1.04008051]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    C_val = train(x_val, y_val)\n",
    "    print('Cost {:} at iteration {}'.format(C_val,i))\n",
    "print(b.get_value())\n",
    "print(W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation graph and loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `A` be a tensor and `k` a positive integer, we are interested by computing `A.^k` (element-wise power). This computation involves a for loop that iterates `k` times. To define the associated computation graph, we use the `scan` function. The arguments of the `scan` function are the:\n",
    "- fn: a Theano function representing the computation that is done in a single iteration of the loop represented by the scan op. Note that **the order of parameters is fixed by** `scan`: the output of the prior call to `fn` is the first parameter, followed by all non-sequences,\n",
    "- output_info: initial value of the output variable,\n",
    "- sequence: A sequence is a Theano variable which Scan will iterate over and give sub-elements to its inner function as input. A sequence has no associated output. For a sequence variable X, at timestep t, the inner function will receive as input the sequence element X[t],\n",
    "- non_sequences: A non-sequence is a Theano variable which Scan will provide as-is to its inner function. Like a sequence, a non-sequence has no associated output. For a non-sequence variable X, at timestep t, the inner function will receive as input the variable X,\n",
    "- n_steps: number of iteration\n",
    "\n",
    "Then, `scan` returns a tuple containing our result and a dictionary of updates (empty in the following case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(range(10),2))\n",
    "print(power(range(10),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scan, it is important to distinguish the following types of output variables (tap refers to time slices of sequences or outputs):\n",
    "- Nitsot (no input tap, single output tap) : A nitsot is an output variable of the inner function that is not fed back as an input to the next iteration of the inner function. Nitsots are typically encountered in situations where Scan is used to perform a ‘map’ operation (every element in a tensor is independently altered using a given operation to produce a new tensor) such as squaring every number in a vector,\n",
    "- Sitsot (single input tap, single output tap) : A sitsot is an output variable of the inner function that is fed back as an input to the next iteration of the inner function. A typical setting where a sitsot might be encountered is the case where Scan is used to compute the cumulative sum over the elements of a vector and a sitsot output is employed to act as an accumulator,\n",
    "- Mitsot (multiple input taps, single output tap) : A mitsot is an output variable of the inner function that is fed back as an input to future iterations of the inner function (either multiple future iterations or a single one that isn’t the immediate next one). For example, a mitsot might be used in the case where Scan is used to compute the Fibonacci sequence, one term of the sequence at every timestep, since every computed term needs to be reused to compute the two next terms of the sequence,\n",
    "- Mitmot (multiple input taps, multiple output taps) : These outputs exist but they cannot be directly created by the user. They can appear in a theano graph as a result of taking the gradient of the output of a Scan with respect to its inputs: This will result in the creation of a new scan node used to compute the gradients of the first scan node. If the original Scan had sitsots or mitsots variables, the new Scan will use mitmots to compute the gradients through time for these variables.\n",
    "\n",
    "The first three types can be used by the user, while the last one is only used internally for computing the gradient through `scan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we calculate the polynomial by first generating each of the coefficients, and then summing them at the end. There is no accumulation of results so we can set `outputs_info` to `None`. This indicates to scan that it doesn’t need to pass the prior result to `fn`.\n",
    "\n",
    "The general order of function parameters to fn is:\n",
    "- sequences (if any),\n",
    "- prior result(s) (if needed), \n",
    "- non-sequences (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-12173aa50c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_coefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_coefficients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "coefficients = theano.tensor.vector(\"coefficients\")\n",
    "x = T.scalar(\"x\")\n",
    "\n",
    "max_coefficients_supported = 10000\n",
    "\n",
    "# Generate the components of the polynomial\n",
    "components, updates = theano.scan(fn=lambda coefficient, power, free_variable: coefficient * (free_variable ** power),\n",
    "                                  outputs_info=None,\n",
    "                                  sequences=[coefficients, theano.tensor.arange(max_coefficients_supported)],\n",
    "                                  non_sequences=x)\n",
    "# Sum them up\n",
    "polynomial = components.sum()\n",
    "\n",
    "# Compile a function\n",
    "calculate_polynomial = theano.function(inputs=[coefficients, x], outputs=polynomial)\n",
    "\n",
    "# Test\n",
    "test_coefficients = numpy.asarray([1, 0, 2], dtype=numpy.float32)\n",
    "test_value = 3\n",
    "print(calculate_polynomial(test_coefficients, test_value))\n",
    "print(1.0 * (3 ** 0) + 0.0 * (3 ** 1) + 2.0 * (3 ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information about `scan`, please refer to [theano doc](http://deeplearning.net/software/theano/library/scan.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and debugging\n",
    "## Graph visualization\n",
    "### Comparing `out` with `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.printing import pydotprint\n",
    "from IPython.display import Image, SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(out, format='png', compact=False, return_image=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(out, format='png', return_image=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(predict, format='png', return_image=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `upd_*` with `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint([upd_W, upd_b], format='png', return_image=True), width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(train, format='png', return_image=True), width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `debugprint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.printing import debugprint\n",
    "debugprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debugprint(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
