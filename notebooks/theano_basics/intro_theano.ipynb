{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Theano\n",
    "\n",
    "This notebook contains the code snippets from the slides, so you can execute them and tinker with those examples.\n",
    "\n",
    "To execute a cell: Ctrl-Enter.\n",
    "\n",
    "The code was executed with the default configuration of Theano: `floatX=float64`, `device=cpu` and the configuration for GPU `floatX=float32,device=gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['THEANO_FLAGS'] = 'floatX=float64,device=cpu,mode=FAST_RUN'\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cuda,mode=FAST_RUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce GTX TITAN Black (0000:05:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano concepts\n",
    "\n",
    "## Symbolic input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbolic inputs that you operate on are **Variables** and what you get from applying various **Ops** to these inputs are also Variables. A Variable is the main data structure you work with. A **Type** in Theano represents a set of constraints on potential data objects. These constraints allow Theano to tailor C code to handle them and to statically optimize the computation graph. The Type of both `x` and `y` is `vector`. Here is [the complete list of types](http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72a38b287232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **Op** defines a certain computation on some types of inputs, producing some types of outputs. From a list of input Variables and an Op, you can build an **Apply** node representing the application of the Op to the inputs.\n",
    "\n",
    "An Apply node is a type of internal node used to represent a computation graph.\n",
    "It represents the application of an Op on one or more inputs, where each input is a Variable. By convention, each Op is responsible for knowing how to build an Apply node from a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![apply.png](http://deeplearning.net/software/theano/_images/apply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`theano.function` is the interface for compiling graphs into callable objects. When `theano.function` is executed, the computation graph is optimized and theano generates an efficient code in C (with calls to CUDA if the gpu flag is set). This is totally transparent to the user, except for the different compilation modes.\n",
    "The mode argument controls the sort of optimizations that will be applied to the graph, and the way the optimized graph will be evaluated. These modes are:\n",
    "- `FAST_COMPILE`: Apply just a few graph optimizations and only use Python implementations. So GPU is disabled.\n",
    "\n",
    "- `FAST_RUN`: Apply all optimizations and use C implementations where possible. (DEFAULT)\n",
    "\n",
    "- `DebugMode`: Verify the correctness of all optimizations, and compare C and Python implementations. This mode can take much longer than the other modes, but can identify several kinds of problems.\n",
    "\n",
    "The default is typically `FAST_RUN` but this can be changed in `theano.config.mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65080136, -1.82465468,  0.38440389]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(1, 3)\n",
    "b = np.random.randn(1, 3)\n",
    "\n",
    "# theano.function([inputs], [outputs])\n",
    "f = theano.function([x, y], z)\n",
    "f(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Shared Variable** is a hybrid symbolic and non-symbolic variable whose value may be shared between multiple functions. Shared variables can be used in symbolic expressions but they also have an internal value that defines the value taken by this symbolic variable in all the functions that use it. The value can be accessed and modified by the .get_value() and .set_value() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "W_val = np.random.randn(4, 3)\n",
    "b_val = np.ones(3)\n",
    "\n",
    "W = theano.shared(W_val)\n",
    "b = theano.shared(b_val)\n",
    "\n",
    "W.name = 'W'\n",
    "b.name = 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49671415 -0.1382643   0.64768854]\n",
      " [ 1.52302986 -0.23415337 -0.23413696]\n",
      " [ 1.57921282  0.76743473 -0.46947439]\n",
      " [ 0.54256004 -0.46341769 -0.46572975]]\n",
      "Before  [ 1.  1.  1.]\n",
      "After  [ 1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(W.get_value())\n",
    "print('Before ', b.get_value())\n",
    "# b.set_value(1) --  Type error, must be a numpy array of shape (3,)\n",
    "# b.set_value(np.array([[1,2],[3,4]])) # Type error, must be a numpy array of shape (3,)\n",
    "b.set_value(np.array([1,2,3]))\n",
    "print('After ', b.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared variables can be used to represent an internal state of a function. In order to modify this internal state, the function has an argument called `updates`, which takes an iterable over pairs (shared_variable, new_expression) List, tuple or dict.\n",
    "\n",
    "Note in the following that `state` is an implicit input of the function `accumulator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = theano.shared(0)\n",
    "inc = T.iscalar('inc')\n",
    "accumulator = theano.function([inc], state, updates=[(state, state+inc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is evaluated and then, the update mechanism is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call to accumulator 0:\n",
      "Second call to accumulator 1:\n",
      "Third call to accumulator 11:\n"
     ]
    }
   ],
   "source": [
    "print('First call to accumulator {}:'.format(accumulator(1)))\n",
    "print('Second call to accumulator {}:'.format(accumulator(10)))\n",
    "print('Third call to accumulator {}:'.format(accumulator(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may happen that you expressed some formula using a shared variable, but you do not want to use its value. In this case, you can use the givens parameter of function which replaces a particular node in a graph for the purpose of one particular function. The givens parameter can be used to replace any symbolic variable, not just a shared variable. You can replace constants, and expressions, in general. Be careful though, not to allow the expressions introduced by a givens substitution to be co-dependent, the order of substitution is not defined, so the substitutions have to work in any order.\n",
    "\n",
    "In practice, a good way of thinking about the givens is as a mechanism that allows you to replace any part of your formula with a different expression that evaluates to a tensor of same shape and dtype. [[reference]](http://deeplearning.net/software/theano/tutorial/examples.html#basictutexamples)\n",
    "\n",
    "In the following, we create a function that takes a scalar `foo`, replace temporarily the variable `state` and return its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "foo = T.scalar(dtype=state.dtype)\n",
    "accumulator2 = theano.function([foo], state, givens=[(state, foo)])\n",
    "print(accumulator2(1))  # we're using 3 for the state, not state.value\n",
    "print(state.get_value())  # old state still there, but we didn't use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A regression toy example\n",
    "## Build a simple model\n",
    "The following is a simple linear transformation (out = Wx +b) followed by a nonlinearity (theano.sigmoid). Note that in this example, we are using `allow_input_downcast=True` in order to avoid an error associated to downcasting x_val from a `float64` to a `float32`. Without this parameter, `x_val` must be casted explicitly: `x_val = np.random.rand(4).astype(np.float32)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62879664  0.26366144  0.65480407]\n"
     ]
    }
   ],
   "source": [
    "x = T.vector('x')\n",
    "y = T.vector('y')\n",
    "\n",
    "W_val = np.random.randn(4, 3)\n",
    "b_val = np.ones(3)\n",
    "W = theano.shared(W_val)\n",
    "b = theano.shared(b_val)\n",
    "W.name = 'W'\n",
    "b.name = 'b'\n",
    "\n",
    "dot = T.dot(x, W)\n",
    "out = T.nnet.sigmoid(dot + b)\n",
    "\n",
    "predict = theano.function([x], out, allow_input_downcast=True)\n",
    "x_val = np.random.rand(4)\n",
    "print(predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, we define a cost function that will evaluate how far the model is from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8978207071266204\n"
     ]
    }
   ],
   "source": [
    "C = ((out - y) ** 2).sum()\n",
    "C.name = 'C'\n",
    "error = theano.function([out, y], C, allow_input_downcast=True)\n",
    "\n",
    "y_val = np.random.uniform(size=3)\n",
    "print(error([0.942, 0.737, 0.676], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is defined, we can compute the gradient of the cost C w.r.t some parameters (W,b). The gradient must be applied to a scalar expression, e.g., the cost C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theano.grad(exp, [Variable])\n",
    "dC_dW, dC_db = theano.grad(C, [W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the gradients, we define the gradient descent update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upd_W = W - 1 * dC_dW\n",
    "upd_b = b - 1 * dC_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compile the expressions and the update rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.]\n",
      "[[ 0.24196227 -1.91328024 -1.72491783]\n",
      " [-0.56228753 -1.01283112  0.31424733]\n",
      " [-0.90802408 -1.4123037   1.46564877]\n",
      " [-0.2257763   0.0675282  -1.42474819]]\n"
     ]
    }
   ],
   "source": [
    "train = theano.function([x, y], C, updates=[(W, upd_W), (b, upd_b)], allow_input_downcast=True)\n",
    "print(b.get_value())\n",
    "print(W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate the gradient descent update rule in order to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 0.8839545789377801 at iteration 0\n",
      "Cost 0.6130558486752022 at iteration 1\n",
      "Cost 0.39498368304054754 at iteration 2\n",
      "Cost 0.25302799106603624 at iteration 3\n",
      "Cost 0.1712113765834918 at iteration 4\n",
      "Cost 0.12371671021543038 at iteration 5\n",
      "Cost 0.09430534609823188 at iteration 6\n",
      "Cost 0.07482420668628094 at iteration 7\n",
      "Cost 0.061180475419945105 at iteration 8\n",
      "Cost 0.05119359754957405 at iteration 9\n",
      "Cost 0.04362302365370288 at iteration 10\n",
      "Cost 0.03772039433598877 at iteration 11\n",
      "Cost 0.03301124616143668 at iteration 12\n",
      "Cost 0.02918193203087368 at iteration 13\n",
      "Cost 0.02601768013726408 at iteration 14\n",
      "Cost 0.023366956499740757 at iteration 15\n",
      "Cost 0.021120066580861643 at iteration 16\n",
      "Cost 0.019195821068913793 at iteration 17\n",
      "Cost 0.01753295352634536 at iteration 18\n",
      "Cost 0.016084437510236114 at iteration 19\n",
      "Cost 0.014813628204898554 at iteration 20\n",
      "Cost 0.013691583930718693 at iteration 21\n",
      "Cost 0.012695169464471471 at iteration 22\n",
      "Cost 0.011805688834688993 at iteration 23\n",
      "Cost 0.011007883816981144 at iteration 24\n",
      "[-0.45587444  2.76836272  1.90992997]\n",
      "[[-0.62051894 -0.86567643 -1.18586207]\n",
      " [-0.6299135  -0.93068994  0.35651396]\n",
      " [-1.79253308 -0.33794406  2.01847203]\n",
      " [-0.47403801  0.3690767  -1.26958318]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    C = train(x_val, y_val)\n",
    "    print('Cost {:} at iteration {}'.format(C,i))\n",
    "print(b.get_value())\n",
    "print(W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation graph and loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `A` be a tensor and `k` a positive integer, we are interested by computing `A.^k` (element-wise power). This computation involves a for loop that iterates `k` times. To define the associated computation graph, we use the `scan` function. The arguments of the `scan` function are the:\n",
    "- fn: a Theano function representing the computation that is done in a single iteration of the loop represented by the scan op. Note that **the order of parameters is fixed by** `scan`: the output of the prior call to `fn` is the first parameter, followed by all non-sequences,\n",
    "- output_info: initial value of the output variable,\n",
    "- sequence: A sequence is a Theano variable which Scan will iterate over and give sub-elements to its inner function as input. A sequence has no associated output. For a sequence variable X, at timestep t, the inner function will receive as input the sequence element X[t],\n",
    "- non_sequences: A non-sequence is a Theano variable which Scan will provide as-is to its inner function. Like a sequence, a non-sequence has no associated output. For a non-sequence variable X, at timestep t, the inner function will receive as input the variable X,\n",
    "- n_steps: number of iteration\n",
    "\n",
    "Then, `scan` returns a tuple containing our result and a dictionary of updates (empty in the following case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print(power(range(10),2))\n",
    "print(power(range(10),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scan, it is important to distinguish the following types of output variables (tap refers to time slices of sequences or outputs):\n",
    "- Nitsot (no input tap, single output tap) : A nitsot is an output variable of the inner function that is not fed back as an input to the next iteration of the inner function. Nitsots are typically encountered in situations where Scan is used to perform a ‘map’ operation (every element in a tensor is independently altered using a given operation to produce a new tensor) such as squaring every number in a vector,\n",
    "- Sitsot (single input tap, single output tap) : A sitsot is an output variable of the inner function that is fed back as an input to the next iteration of the inner function. A typical setting where a sitsot might be encountered is the case where Scan is used to compute the cumulative sum over the elements of a vector and a sitsot output is employed to act as an accumulator,\n",
    "- Mitsot (multiple input taps, single output tap) : A mitsot is an output variable of the inner function that is fed back as an input to future iterations of the inner function (either multiple future iterations or a single one that isn’t the immediate next one). For example, a mitsot might be used in the case where Scan is used to compute the Fibonacci sequence, one term of the sequence at every timestep, since every computed term needs to be reused to compute the two next terms of the sequence,\n",
    "- Mitmot (multiple input taps, multiple output taps) : These outputs exist but they cannot be directly created by the user. They can appear in a theano graph as a result of taking the gradient of the output of a Scan with respect to its inputs: This will result in the creation of a new scan node used to compute the gradients of the first scan node. If the original Scan had sitsots or mitsots variables, the new Scan will use mitmots to compute the gradients through time for these variables.\n",
    "\n",
    "The first three types can be used by the user, while the last one is only used internally for computing the gradient through `scan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we calculate the polynomial by first generating each of the coefficients, and then summing them at the end. There is no accumulation of results so we can set `outputs_info` to `None`. This indicates to scan that it doesn’t need to pass the prior result to `fn`.\n",
    "\n",
    "The general order of function parameters to fn is:\n",
    "- sequences (if any),\n",
    "- prior result(s) (if needed), \n",
    "- non-sequences (if any).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "coefficients = theano.tensor.vector(\"coefficients\")\n",
    "x = T.scalar(\"x\")\n",
    "\n",
    "max_coefficients_supported = 10000\n",
    "\n",
    "# Generate the components of the polynomial\n",
    "components, updates = theano.scan(fn=lambda coefficient, power, free_variable: coefficient * (free_variable ** power),\n",
    "                                  outputs_info=None,\n",
    "                                  sequences=[coefficients, theano.tensor.arange(max_coefficients_supported)],\n",
    "                                  non_sequences=x)\n",
    "# Sum them up\n",
    "polynomial = components.sum()\n",
    "\n",
    "# Compile a function\n",
    "calculate_polynomial = theano.function(inputs=[coefficients, x], outputs=polynomial)\n",
    "\n",
    "# Test\n",
    "test_coefficients = numpy.asarray([1, 0, 2], dtype=numpy.float32)\n",
    "test_value = 3\n",
    "print(calculate_polynomial(test_coefficients, test_value))\n",
    "print(1.0 * (3 ** 0) + 0.0 * (3 ** 1) + 2.0 * (3 ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information about `scan`, please refer to [theano doc](http://deeplearning.net/software/theano/library/scan.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and debugging\n",
    "## Graph visualization\n",
    "### Comparing `out` with `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.printing import pydotprint\n",
    "from IPython.display import Image, SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(out, format='png', compact=False, return_image=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(out, format='png', return_image=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(predict, format='png', return_image=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `upd_*` with `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint([upd_W, upd_b], format='png', return_image=True), width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(pydotprint(train, format='png', return_image=True), width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `debugprint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.printing import debugprint\n",
    "debugprint(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debugprint(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
