{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is highly inspired from \n",
    "- [LSTM Text Generation](https://github.com/Lasagne/Recipes/blob/master/examples/lstm_text_generation.py)\n",
    "- [Lasagne doc about recurrent](http://lasagne.readthedocs.io/en/latest/modules/layers/recurrent.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX TITAN Black (0000:03:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "seed = 1\n",
    "lasagne.random.set_rng(np.random.RandomState(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are hyperparameters, except for `PRINT_FREQ`, that will have an impact on the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequence Length\n",
    "SEQ_LENGTH = 20\n",
    "\n",
    "# Number of units in the hidden (LSTM) layers\n",
    "DEPTH = 2\n",
    "N_HIDDEN = 512\n",
    "NON_LINEARITY = lasagne.nonlinearities.rectify\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# How often should we check the output?\n",
    "PRINT_FREQ = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer to be used for the training. An optimizer can be seen as a function that takes a gradient, obtained by backpropagation, and returns an update to be applied to the current parameters. Other optimizers can be found in: [optimizer reference](http://lasagne.readthedocs.io/en/latest/modules/updates.html?highlight=update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_optimizer = lambda loss, params: lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loads the dataset (the full text *Beyond Good and Evil* by Friedrich Nietzsche) directly from Internet.\n",
    "You can also replace the dataset by your own by adapting the commented line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which t\n"
     ]
    }
   ],
   "source": [
    "import urllib.request #For downloading the sample text file. You won't need this if you are providing your own file.\n",
    "try:\n",
    "    in_text = urllib.request.urlopen('https://s3.amazonaws.com/text-datasets/nietzsche.txt').read()\n",
    "    #in_text = open('your_file.txt', 'r').read()\n",
    "    in_text = in_text.decode(\"utf-8\")\n",
    "    print(in_text[:250])\n",
    "except Exception as e:\n",
    "    print(\"Please verify the location of the input file/URL.\")\n",
    "    print(\"A sample txt file can be downloaded from https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "    raise IOError('Unable to Read Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing consists in retrieving the list of symbols occuring in the text and to convert each of them into an unique index. This index will be used to create an one-hot representation of the symbol that will be the input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(in_text))\n",
    "data_size, vocab_size = len(in_text), len(chars)\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print('Number of unique symbols: {}'.format(vocab_size))\n",
    "print('Number of symbols in the dataset: {}'.format(data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary function creates a minibatch in a 3D tensor (batch_size, SEQ_LENGTH, vocab_size).\n",
    "For each datapoint (fixed first coordinate of the 3D matrix), there is a matrix of dimension (SEQ_LENGTH, vocab_size)\n",
    "where each line contains the one-hot vector representing the character at the associated position. Notice that the sequences have all the same length (SEQ_LENGTH), which can covers many sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (None, None, vocab_size)\n",
    "\n",
    "def iterate_minibatch(\n",
    "    p, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    data=in_text, \n",
    "    return_target=True):\n",
    "    \"\"\"\n",
    "    Return a minibatch compatible with the input of the model and the associated targets\n",
    "    \n",
    "    :type p: int\n",
    "    :param The index of the character to begin to read\n",
    "    :type batch_size: int\n",
    "    :param The number of datapoints in the current batch\n",
    "    :type data: str\n",
    "    :param The whole text\n",
    "    :type return_target: bool\n",
    "    :param Create the targets (next character) associated to the sequences\n",
    "    \"\"\"\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent layers can be used similarly to feed-forward layers except that the input shape is expected to be (batch_size, sequence_length, num_inputs). By setting the first two dimensions as None, we are allowing them to vary. They correspond to batch size and sequence length, so we will be able to feed in batches of varying size with sequences of varying length. If `only_return_final` is set, it only returns the final sequential output (e.g. for tasks where a single target value for the entire sequence is desired). In this case, Theano makes an optimization, which saves memory. If you are working with variable size sequences, an additional parameters `masks` of size (batch_size, sequence_length) is given as a boolean mask where its entries are fixed to 0 after the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm(\n",
    "    vocab_size,\n",
    "    input_shape,\n",
    "    input_var = None,\n",
    "    nonlinearity = lasagne.nonlinearities.tanh,\n",
    "    depth=2, \n",
    "    n_hidden=800,\n",
    "    grad_clip=100):\n",
    "    \"\"\"\n",
    "    A generic function for creating a LSTM neural network.\n",
    "\n",
    "    :type vocab_size: int\n",
    "    :param vocab_size: number of elements in the dictionary\n",
    "    :type input_shape: tuple\n",
    "    :param input_shape: a tuple containing the shape of the input\n",
    "    :type input_var: theano.tensor.var.TensorVariable\n",
    "    :param input_var: a theano symbolic variable, created automatically if None\n",
    "    :type nonlinearity: lasagne.nonlinearities\n",
    "    :param nonlinearity: a nonlinearity function that follows all dense layers\n",
    "    :type depth: int\n",
    "    :param depth: the depth of the LSTM\n",
    "    :type n_hidden: int or list\n",
    "    :param n_hidden: number of hidden units per LSTM cells (if int, the same for all layers)\n",
    "    :type grad_clip: float\n",
    "    :param grad_clip: threshold for the gradient in the LSTM\n",
    "   \"\"\"\n",
    "\n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, SEQ_LENGTH, num_features)\n",
    "    network = lasagne.layers.InputLayer(\n",
    "        shape=input_shape,\n",
    "        input_var=input_var\n",
    "    )\n",
    "\n",
    "    # We now build the LSTM layer\n",
    "    # We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. \n",
    "    for _ in range(depth-1):\n",
    "    \n",
    "        network = lasagne.layers.LSTMLayer(\n",
    "            network, \n",
    "            num_units=n_hidden, \n",
    "            grad_clipping=grad_clip,\n",
    "            nonlinearity=nonlinearity)\n",
    "\n",
    "    network = lasagne.layers.LSTMLayer(\n",
    "        network, \n",
    "        num_units=n_hidden, \n",
    "        grad_clipping=grad_clip,\n",
    "        nonlinearity=nonlinearity,\n",
    "        only_return_final=True)\n",
    "\n",
    "    # The output the previous module, with shape (batch_size, N_HIDDEN),\n",
    "    # is then passed through the softmax nonlinearity to \n",
    "    # create a probability distribution over the dictionary.\n",
    "    # The output of this stage is (batch_size, vocab_size).\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=vocab_size, W = lasagne.init.Normal(), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we want to maximize the probability to output the right character given the SEQ_LENGTH previous ones. To do this, we retrieve the output of our model, which is a softmax over the characters, and we compare it to the actual character of the sequence. Finally, since we are using minibatches of size `BATCH_SIZE`, we compute the mean over the examples of the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano tensor for the targets\n",
    "input_var = T.tensor3('inputs')\n",
    "target_values = T.ivector('target_output')\n",
    "\n",
    "network = create_lstm(\n",
    "    vocab_size,\n",
    "    input_shape,\n",
    "    input_var,\n",
    "    nonlinearity=NON_LINEARITY,\n",
    "    depth=DEPTH, \n",
    "    n_hidden=N_HIDDEN)\n",
    "\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "network_output = lasagne.layers.get_output(network)\n",
    "\n",
    "# The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "loss = lasagne.objectives.categorical_crossentropy(network_output, target_values).mean()\n",
    "\n",
    "# Retrieve all the parameters of the models and create the optimizer\n",
    "params = lasagne.layers.get_all_params(network,trainable=True)\n",
    "updates = my_optimizer(loss, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theano functions for training and computing cost\n",
    "print(\"Compiling functions ...\")\n",
    "train = theano.function([input_var, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "compute_loss = theano.function([input_var, target_values], loss, allow_input_downcast=True)\n",
    "\n",
    "# In order to generate text from the network, we need the probability distribution of the next character given\n",
    "# the state of the network and the input (a seed).\n",
    "# In order to produce the probability distribution of the prediction, we compile a function called probs.\n",
    "probs = theano.function([input_var],network_output,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function generates text given a phrase of length at least `SEQ_LENGTH`.\n",
    "The phrase is set using the variable generation_phrase.\n",
    "The optional input `N` is used to set the number of characters of text to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generation_phrase = \"The meaning of life is\" #This phrase will be used as seed to generate text.\n",
    "\n",
    "def predict(N=200):\n",
    "    \"\"\"\n",
    "    Output a sequence of characters of lenght N according to the current model\n",
    "    \n",
    "    :type N: int\n",
    "    :param N: the number of characters to generate\n",
    "    \"\"\"\n",
    "    assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "    sample_ix = []\n",
    "    x,_ = iterate_minibatch(len(generation_phrase)-SEQ_LENGTH, 1, generation_phrase,0)\n",
    "\n",
    "    for i in range(N):\n",
    "        # Pick the character that got assigned the highest probability\n",
    "        # ix = np.argmax(probs(x).ravel())\n",
    "        \n",
    "        # Alternatively, to sample from the distribution instead:\n",
    "        ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "        sample_ix.append(ix)\n",
    "        x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "        x[:,SEQ_LENGTH-1,:] = 0\n",
    "        x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "\n",
    "    random_snippet = generation_phrase + ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "    print(\"----\\n %s \\n----\" % random_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Seed used for text generation is: The meaning of life is\n",
      "----\n",
      " The meaning of life isS\"Eé:Ra(h6fW9rclR7f]fYxjh0CweeUFVzCZqi6yo]3p-HX)XLmLP,9zjkXn1VAK)n-wF--AL43a'æpkryodPcD02N\"WW0[U.ySTBj4AIH-c?kXYQc(4)3SEXKguth.ëLpfROwndhHæv[220PI2BXd(EVR,LUP2p-é(Æv_?R9bZHcE0vw)ztæ,NRc9Lk=02jkp6(5M\"L \n",
      "----\n",
      "Epoch 0.0 average loss = 3.2399878993034363\n",
      "----\n",
      " The meaning of life isi ol nn ihonybtielseS\n",
      "h ta o  fEd essupltyrtefa dronirarr mieearvesiRttorenUdr  leate;aro,ytr,trstenrn tiU w .toiittrisue evntsnt ipip ywtt\"da eTglvhainsy net inhoi den sneteohtm olef\"et p ctn rt teha \n",
      "----\n",
      "Epoch 0.10650814704115374 average loss = 3.039201445102692\n",
      "----\n",
      " The meaning of life is ileur soutens \n",
      "8leate\n",
      "dtaendsy, teo vmdenuTtutiay aor ,d,, enregts eiwEd eemt nh iae, -o\n",
      "etstden\n",
      "nfaaards ifeattesl,uIu7nthil, as=aetvlhie ëed ogia tfnuchpoachac8\"tissioe paate nee, anr iadeoc\"spoj5g \n",
      "----\n",
      "Epoch 0.2130162940823075 average loss = 2.7900235023498534\n",
      "----\n",
      " The meaning of life ismseguosisd Nast\n",
      "ely teed!tsSen af therdirany RJ cerfituim so l:e, igitomches- tomirase aisw sasucs rorirk.eunpifiled al lol, ahul aritid wgomhuss tilnogerrd doUermhed\n",
      "lewrencf romore,-hprsDatorirI-att \n",
      "----\n",
      "Epoch 0.31952444112346123 average loss = 2.6640562987327576\n",
      "----\n",
      " The meaning of life is lhirmfron afd, apeuis. er samsf and ortthe fhalse-dacticbognt\n",
      "katneply thos sfeenc olel- fecvage, cerClet an nasy mith tass.yitoE, mhemtev\" fherd-\n",
      "wot ottiro 'M tfochg pocerst. wp did tiuh bedrtomanl \n",
      "----\n",
      "Epoch 0.426032588164615 average loss = 2.5905228476524353\n",
      "----\n",
      " The meaning of life is forpthedesvarnentelly, !od ameek an in eetey\n",
      "alite only ardircise lace aclor, w. bamiritalan- hheve oulledsad malterof tho warebssithes\n",
      "onrslrosginx Eo henSedard afer:erbeuce, socad losAl-oungeS ic o \n",
      "----\n",
      "Epoch 0.5325407352057687 average loss = 2.4818902339935303\n",
      "----\n",
      " The meaning of life isstger and the\n",
      "whian tad mimamile bheun\n",
      "\n",
      "ond\n",
      "roo decicestuco2ion os thaun werorh itas\n",
      "rintir tham ke alm aos bor wiens. TBes\n",
      "osnetis, ir walt tre gimeras! girted Aoks. wind ol\n",
      "ovy eo tiot o\n",
      ": fypRoml f \n",
      "----\n",
      "Epoch 0.6390488822469225 average loss = 2.4089879932403564\n",
      "----\n",
      " The meaning of life is ot of arkned pha thin desusy Wicrauther, en thit o tuosged the\n",
      "icagy ?tithe asley mace lige cecunteviviat. fere)gave\n",
      "the\n",
      "tho licerletGer. the er tho goren=union id cesalalot wacle aps-dail om whace s \n",
      "----\n",
      "Epoch 0.7455570292880762 average loss = 2.3199677107334136\n",
      "----\n",
      " The meaning of life is. hellerge tamses arl tiorituce the this is to hof arcos ele andeeve of caltiams taex oreghe thee tilel censrey,\n",
      "acaneaheny wounj hinl, Te to\n",
      "nedeis toitk Mo fo aodsticand fiss hy tirlipyirioipitions  \n",
      "----\n",
      "Carriage Return\n",
      "Epoch 0.85206517632923 average loss = 2.3630700902938844\n",
      "----\n",
      " The meaning of life is tho haruthow\n",
      "rhis se haN Sular an it Fl mige no co the himeery Wq to trelrstare ig ther aid efanrevcisurg inviritaly or ape if locbaconse eficisysw\n",
      "and of of that ovlar an milpay golhMifass ofer\" Hre \n",
      "----\n",
      "Epoch 0.9585733233703837 average loss = 2.336587360858917\n",
      "----\n",
      " The meaning of life is coujly\n",
      "Inevm. hE I3i3u..\n",
      "-IP OoC spinus a mathert a tad.,\n",
      "Tu whemestilg-\n",
      "whoon shla? Abing the peres--ve\n",
      "rithen--of ig ches is a drisuciof- hit hot fom\n",
      "the Cacenaus and braresd thingidity the\n",
      "s\n",
      "apty  \n",
      "----\n",
      "Epoch 1.0650814704115374 average loss = 2.269758390903473\n",
      "----\n",
      " The meaning of life is fomeAm is the fered,- sempmane,, which pratine sole in theer droaldre an sfien\n",
      "theughy mensspe asle trith wiml, thet tation,\n",
      "thes exerutive ot the ppevire? wist bemeco, remesey cans and letees, toric \n",
      "----\n",
      "Epoch 1.1715896174526912 average loss = 2.255251970052719\n",
      "----\n",
      " The meaning of life is onl to\n",
      "in pos at fag longes, \"vey TR mans giln nok\n",
      "Is Eucwo must is\n",
      "acjmclit arY with of chon the maq the8, SR A\n",
      "EmEII NO. In\n",
      "=uæpiph\n",
      "pressp abond \"orgsing the\n",
      "or\n",
      "the an ha dast bbio?y\n",
      "tele sonial th \n",
      "----\n",
      "Epoch 1.278097764493845 average loss = 2.2188403055667876\n",
      "----\n",
      " The meaning of life is asceated,, the Ts -aze mote, an Rfyerd is epoaone--no beeiots fures, inssid andariof, difpe ficaess negofutie of as is svosifyy of the solss proul cuofy,-doucsemand.- hat menE BOneteres int that\n",
      "rry  \n",
      "----\n",
      "Epoch 1.3846059115349987 average loss = 2.174964720726013\n",
      "----\n",
      " The meaning of life is colnenging-dunts dat blenobly chaved. B2\n",
      "wepition-:anulhal beach Fovlacicnlysal. Wich of pelwuns, of thomeding is adualisations inscinaciense of\n",
      "has rediecal endigation\n",
      "ar pall be than ther, istelled \n",
      "----\n",
      "Epoch 1.4911140585761524 average loss = 2.1232355549335478\n",
      "----\n",
      " The meaning of life is bedurtupbe tre misterfalesnaunted ot it umpmidantty.--a hooper ant muspuved of\n",
      "the conlyity sorcmicinnst-\"uithings in thispe aliwy and por semfice jom the cacputait. Swith-\n",
      "Heleidenge of\n",
      "marferiotsai \n",
      "----\n",
      "Epoch 1.5976222056173062 average loss = 2.0729115312099458\n",
      "----\n",
      " The meaning of life is list\n",
      "there lateros hy nothe -nowy withs it dayss himle of dains\n",
      "pnvoss dodles ams unalrerable to as as the shiks to cofpextand raid elecanve the erituliences is dasuom the bonder His\n",
      "comong to\n",
      "stible \n",
      "----\n",
      "Carriage Return\n",
      "Epoch 1.70413035265846 average loss = 2.098266562461853\n",
      "----\n",
      " The meaning of life is to is, wall woo han ane sto pive\n",
      "clots in that, is imsasfle ithirs and ancaop)\n",
      "not alpuring,\n",
      "Hectrition whom A. Chur belame that rittrr-bchan prowinanding teore ispistite speve beto the was mual rust \n",
      "----\n",
      "Epoch 1.8106384996996137 average loss = 2.103282114028931\n",
      "----\n",
      " The meaning of life is caxtility, mive sorellysR--on the sorat of earaire, who leghtaraty inceivivy of\n",
      "the wortatuly to the\n",
      "mentify and\n",
      "morust to \n",
      "O OLOLCE T5EcSIUGLGEmEB. This\n",
      "-TTOLNI\n",
      "AU.H one axsiof\" portagves in the sof \n",
      "----\n",
      "Epoch 1.9171466467407674 average loss = 2.067755512714386\n",
      "----\n",
      " The meaning of life is no laint he\n",
      "\" who is bricfsdation, and hasf SUU4I1? 3cE -ath\n",
      "hor' pirss of ard bacie -vall lof and for then: She hind let\n",
      "mese--Pnothing,\n",
      "\"TE S1AEDAU he ang EfEe\n",
      "footen vite in bleare to the ussve ra \n",
      "----\n",
      "Epoch 2.023654793781921 average loss = 2.0794330422878264\n",
      "----\n",
      " The meaning of life is egwrer evee of ard succeay dom WORELE\n",
      "Iliyat zvhaldeven. Ono\n",
      "ass Thound and \"gro my ulwolgts heus rare masse fow which\n",
      "weal to tuR DEOlezes eveots afwe gerebtriog, that wom cughtomex seoppent and atw \n",
      "----\n",
      "Epoch 2.130162940823075 average loss = 2.0464156181812285\n",
      "----\n",
      " The meaning of life is cestiget to the\n",
      "sGen pration,\n",
      "wech storans enoures aring most of all sugr har hivposmecs in thay renne owqhy has nithel of arle re wanter, to al to in\n",
      "-u, forsigar tich\n",
      "bice the toke pessut for-lenne \n",
      "----\n",
      "Epoch 2.2366710878642286 average loss = 2.0318523936271666\n",
      "----\n",
      " The meaning of life is absun tran arond beerow- for hatbertiols acprrais his utsald an ver of que wat be assess\n",
      "nemumses fhat side anesy\" praners? was in, wowh ener he not ang rasured bo tho whal panerifatled\n",
      "teratee of, l \n",
      "----\n",
      "Epoch 2.3431792349053824 average loss = 1.9496320095062256\n",
      "----\n",
      " The meaning of life is the excoudibe mesuotinatelf-Fu0uallys ores deaur debpioust in which a tevepait fure at not the scadien to ce troe ach heact orment, bain\n",
      "the fory him\n",
      "alveripars blisal by EETTBy-\n",
      "The soul wa porsecue \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "print(\"Seed used for text generation is: \" + generation_phrase)\n",
    "p = 0\n",
    "for it in range(int(data_size * NUM_EPOCHS / BATCH_SIZE)):\n",
    "    predict() # Generate text using the p^th character as the start. \n",
    "            \n",
    "    avg_cost = 0;\n",
    "    for i in range(PRINT_FREQ):\n",
    "        x,y = iterate_minibatch(p)\n",
    "\n",
    "        p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "        if(p+BATCH_SIZE+SEQ_LENGTH >= data_size):\n",
    "            print('Carriage Return')\n",
    "            p = 0;\n",
    "\n",
    "        avg_cost += train(x, y)\n",
    "    print(\"Epoch {} average loss = {}\".format(it*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
